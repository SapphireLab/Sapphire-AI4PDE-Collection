# TransNet

## 基本信息

- 标题: TransNet: Transferable Neural Networks for Partial Differential Equations
- 作者: 
  1. [Zezhong Zhang](../Authors/Zezhong_Zhang.md)
  2. [Feng Bao](../Authors/Feng_Bao.md)
  3. [Lili Ju](../Authors/Lili_Ju_鞠立力.md)
  4. [Guannan Zhang](../Authors/Guannan_Zhang.md)
- 预印: [ArXiv](https://arxiv.org/abs/2301.11701v1)
- 发表: [Springer: Journal of Scientific Computing](../Publications/Springer-JSC.md) (2024) Vol.99 No.2 | [DOI](https://doi.org/10.1007/s10915-024-02463-y) 
- 谷歌: [Scholar](https://scholar.google.com/scholar?cluster=104089547461515275)
- 时点: 
  - 2023.01.27 Preprint (v1)
  - 2023.05.06 Received
  - 2023.11.21 Revised
  - 2024.01.15 Accepted
  - 2024.02.21 Published Online
- 更新:
  - 2024.08.13

## 摘要

<details>
<summary>展开原文</summary>

> Transfer learning for partial differential equations (PDEs) is to develop a pre-trained neural network that can be used to solve a wide class of PDEs. 
> Existing transfer learning approaches require much information about the target PDEs such as its formulation and/or data of its solution for pre-training. 
> In this work, we propose to design transferable neural feature spaces for the shallow neural networks from purely function approximation perspectives without using PDE information. 
> The construction of the feature space involves the re-parameterization of the hidden neurons and uses auxiliary functions to tune the resulting feature space. 
> Theoretical analysis shows the high quality of the produced feature space, i.e., uniformly distributed neurons. 
> We use the proposed feature space as the pre-determined feature space of a random feature model, and use existing least squares solvers to obtain the weights of the output layer.
> Extensive numerical experiments verify the outstanding performance of our method, including significantly improved transferability, e.g., using the same feature space for various PDEs with different domains and boundary conditions, and the superior accuracy, e.g., several orders of magnitude smaller mean squared error than the state of the art methods. 

</details>
<br>

用于偏微分方程的迁移学习是指开发一个可以用于求解一大类偏微分方程的预训练神经网络.
现有的迁移学习方法需要大量关于目标偏微分方程的信息, 例如它的具体形式和/或其解的数据进行预训练.
在这项工作中, 我们提出为浅层神经网络设计可迁移的神经特征空间, 这些空间是从纯粹的函数逼近角度出发构建的, 无需偏微分方程信息.
特征空间的构造涉及到隐藏神经元的重参数化和使用辅助函数来调整生成的特征空间.
理论分析表明构造的特征空间的高质量, 即均匀分布神经元.
我们将所提出的特征空间作为随机特征模型的预定义特征空间, 并使用现有的最小二乘求解器来获得输出层的权重.

大量的数值实验验证了我们方法的优越性能, 包括两点:
1. 显著提高的可迁移性, 例如求解具有不同定义域和边界条件的各种偏微分方程时使用相同特征空间;
2. 优越的精度, 例如比现有方法的均方误差小了几个数量级.


## 5.总结

<details>
<summary>展开原文</summary>

> We propose a transferable neural network model based on the shallow neural networks to advance the state-of-the-art of using neural networks to solve PDEs. 
> The key ingredient is to construct a neural feature space independent of any PDE, which makes it easy to transfer the neural feature space to various PDEs in different domains. 
> Moreover, because the feature space is in fact fixed when using ***TransNet*** to solve a PDE, we only need to solve linear least squares problems, which avoids the drawbacks of SGD-based training algorithms, e.g., ill-conditioning. 
> Numerical experiments show that the proposed ***TransNet*** can exploit more expressive power of a given neural network than the compared baselines. 
> This work is the first scratch in this research direction, and there are multiple potential related topics that will be studied in our future work, including 
> 1. theoretical analysis of the convergence rate of ***TransNet*** in solving PDEs. 
> We observe in Fig.05 that the MSE of ***TransNet*** decays along with the increase of the number of hidden neurons. 
> A natural question to study is that whether ***TransNet*** can achieve the optimal convergence rate of the single-hidden-layer fully-connected neural network. 
> 2. Extension to multi-layer neural networks. 
> Even though the single-hidden-layer model has sufficient expressive power for the PDEs tested in this work, there are more complicated PDEs, e.g., turbulence models, that could require multi-layer models with much higher expressive power. 
> 3. The properties of the least squares problem. 
> In this work, we use the standard least squares solver of Pytorch in the numerical experiments. 
> However, it is worth further investigation of the properties of this specific least squares problem. 
> For example, since the set of neurons $\{\sigma(\bm w_m^{\mathsf{T}}\bm y + b_m)\}^M_{m=1}$ forms a non-orthogonal basis, it is possible to have linearly correlated neurons which will reduce the column rank of the least squares matrix, or even lead to an under-determined system. 
> This will require the use of some regularization techniques, e.g., ridge regression, to stabilize the least squares system. 
> Additionally, compressed sensing, i.e., $l_1$ regularization, could be added to remove redundant neurons from the feature space as needed and obtain a sparse neural network.

</details>
<br>

我们提出了一种基于浅层神经网络的可迁移神经网络模型, 更新了使用神经网络求解偏微分方程的最先进水平.
关键部分是构造一个和任意偏微分方程无关的神经特征空间, 使得它易于将神经特征空间迁移到不同定义域的多种偏微分方程.
此外, 由于使用 ***TransNet*** 求解偏微分方程时, 特征空间实际上是固定的, 因此我们只需要求解线性最小二乘问题, 避免了使用基于随机梯度下降的训练算法的缺陷, 如病态收敛.
数值实验表明, 我们所提出的 ***TransNet*** 能够比对比的基线方法更好地利用给定的神经网络的表达能力.
这项工作是这项研究方向的开端, 还有许多相关的课题将在未来进行研究, 包括:
1. 对 ***TransNet*** 在求解偏微分方程时的收敛率的理论分析.
   我们在图 05 中观察到, ***TransNet*** 的均方误差会随着隐藏神经元数量的增加而逐渐减小.
   一个自然的问题是研究 ***TransNet*** 是否可以达到单隐层全连接神经网络的最优收敛率.
2. 扩展到多层神经网络.
   尽管单隐层模型对于本项工作中测试的偏微分方程具有足够的表达能力, 但更复杂的偏微分方程, 例如湍流模型, 可能需要具有更高表达能力的多层模型.
3. 最小二乘问题的性质.
   在本项工作中, 我们在数值实验中使用了 Pytorch 的标准最小二乘求解器.
   然而, 对于特定的最小二乘问题, 还有许多问题需要进一步研究.
   例如, 由于神经元集合 $\{\sigma(\bm w_m^{\mathsf{T}}\bm y + b_m)\}^M_{m=1}$ 形成了一个非正交基, 因此可能存在线性相关的神经元, 这将导致最小二乘矩阵的列秩降低, 甚至导致一个欠定系统.
   这将需要使用一些正则化技术, 如岭回归, 来稳定最小二乘系统.
   此外, 压缩感知, 即 $l_1$ 正则化, 可以被添加到特征空间中, 以根据需要移除冗余神经元, 获得稀疏神经网络.