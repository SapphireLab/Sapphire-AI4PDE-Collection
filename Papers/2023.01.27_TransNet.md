# TransNet

## 基本信息

- 标题: TransNet: Transferable Neural Networks for Partial Differential Equations
- 作者: 
  1. [Zezhong Zhang](../Authors/Zezhong_Zhang.md)
  2. [Feng Bao](../Authors/Feng_Bao.md)
  3. [Lili Ju](../Authors/Lili_Ju_鞠立力.md)
  4. [Guannan Zhang](../Authors/Guannan_Zhang.md)
- 预印: [ArXiv](https://arxiv.org/abs/2301.11701v1)
- 发表: [Springer: Journal of Scientific Computing](../Publications/Springer-JSC.md) (2024) Vol.99 No.2 | [DOI](https://doi.org/10.1007/s10915-024-02463-y) 
- 谷歌: [Scholar](https://scholar.google.com/scholar?cluster=104089547461515275)
- 时点: 
  - 2023.01.27 Preprint (v1)
  - 2023.05.06 Received
  - 2023.11.21 Revised
  - 2024.01.15 Accepted
  - 2024.02.21 Published Online
- 更新:
  - 2024.08.13

## 摘要

<details>
<summary>展开原文</summary>

> Transfer learning for partial differential equations (PDEs) is to develop a pre-trained neural network that can be used to solve a wide class of PDEs. 
> Existing transfer learning approaches require much information about the target PDEs such as its formulation and/or data of its solution for pre-training. 
> In this work, we propose to design transferable neural feature spaces for the shallow neural networks from purely function approximation perspectives without using PDE information. 
> The construction of the feature space involves the re-parameterization of the hidden neurons and uses auxiliary functions to tune the resulting feature space. 
> Theoretical analysis shows the high quality of the produced feature space, i.e., uniformly distributed neurons. 
> We use the proposed feature space as the pre-determined feature space of a random feature model, and use existing least squares solvers to obtain the weights of the output layer.
> Extensive numerical experiments verify the outstanding performance of our method, including significantly improved transferability, e.g., using the same feature space for various PDEs with different domains and boundary conditions, and the superior accuracy, e.g., several orders of magnitude smaller mean squared error than the state of the art methods. 

</details>
<br>

用于偏微分方程的迁移学习是指开发一个可以用于求解一大类偏微分方程的预训练神经网络.
现有的迁移学习方法需要大量关于目标偏微分方程的信息, 例如它的具体形式和/或其解的数据进行预训练.
在这项工作中, 我们提出为浅层神经网络设计可迁移的神经特征空间, 这些空间是从纯粹的函数逼近角度出发构建的, 无需偏微分方程信息.
特征空间的构造涉及到隐藏神经元的重参数化和使用辅助函数来调整生成的特征空间.
理论分析表明构造的特征空间的高质量, 即均匀分布神经元.
我们将所提出的特征空间作为随机特征模型的预定义特征空间, 并使用现有的最小二乘求解器来获得输出层的权重.

大量的数值实验验证了我们方法的优越性能, 包括两点:
1. 显著提高的可迁移性, 例如求解具有不同定义域和边界条件的各种偏微分方程时使用相同特征空间;
2. 优越的精度, 例如比现有方法的均方误差小了几个数量级.
